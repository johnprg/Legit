model,best_score,fit_time,best_params,best_estimator
Logistic,0.9987373737373737,20.922752618789673,"{'classifier': LogisticRegression(C=10.0, max_iter=10000, penalty='l1', solver='liblinear'), 'classifier__C': 10.0, 'classifier__max_iter': 10000, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear', 'vectorize': TfidfVectorizer(max_features=500,
                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',
                            'ourselves', 'you', ""you're"", ""you've"", ""you'll"",
                            ""you'd"", 'your', 'yours', 'yourself', 'yourselves',
                            'he', 'him', 'his', 'himself', 'she', ""she's"",
                            'her', 'hers', 'herself', 'it', ""it's"", 'its',
                            'itself', ...]), 'vectorize__max_features': 500}","Pipeline(steps=[('preprocess', LemmatizerStemmer()),
                ('vectorize',
                 TfidfVectorizer(max_features=500,
                                 stop_words=['i', 'me', 'my', 'myself', 'we',
                                             'our', 'ours', 'ourselves', 'you',
                                             ""you're"", ""you've"", ""you'll"",
                                             ""you'd"", 'your', 'yours',
                                             'yourself', 'yourselves', 'he',
                                             'him', 'his', 'himself', 'she',
                                             ""she's"", 'her', 'hers', 'herself',
                                             'it', ""it's"", 'its', 'itself', ...])),
                ('classifier',
                 LogisticRegression(C=10.0, max_iter=10000, penalty='l1',
                                    solver='liblinear'))])"
Decision Tree,0.9767676767676767,20.956968784332275,"{'classifier': DecisionTreeClassifier(criterion='entropy', max_depth=20, max_features='sqrt',
                       min_samples_leaf=5, min_samples_split=5), 'classifier__criterion': 'entropy', 'classifier__max_depth': 20, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 5, 'classifier__min_samples_split': 5, 'vectorize': CountVectorizer(max_features=500,
                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',
                            'ourselves', 'you', ""you're"", ""you've"", ""you'll"",
                            ""you'd"", 'your', 'yours', 'yourself', 'yourselves',
                            'he', 'him', 'his', 'himself', 'she', ""she's"",
                            'her', 'hers', 'herself', 'it', ""it's"", 'its',
                            'itself', ...]), 'vectorize__max_features': 500}","Pipeline(steps=[('preprocess', LemmatizerStemmer()),
                ('vectorize',
                 CountVectorizer(max_features=500,
                                 stop_words=['i', 'me', 'my', 'myself', 'we',
                                             'our', 'ours', 'ourselves', 'you',
                                             ""you're"", ""you've"", ""you'll"",
                                             ""you'd"", 'your', 'yours',
                                             'yourself', 'yourselves', 'he',
                                             'him', 'his', 'himself', 'she',
                                             ""she's"", 'her', 'hers', 'herself',
                                             'it', ""it's"", 'its', 'itself', ...])),
                ('classifier',
                 DecisionTreeClassifier(criterion='entropy', max_depth=20,
                                        max_features='sqrt', min_samples_leaf=5,
                                        min_samples_split=5))])"
Bayes,0.9737373737373738,20.876189351081848,"{'classifier': MultinomialNB(alpha=0.1, fit_prior=False), 'classifier__alpha': 0.1, 'classifier__class_prior': None, 'classifier__fit_prior': False, 'vectorize': CountVectorizer(max_features=500,
                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',
                            'ourselves', 'you', ""you're"", ""you've"", ""you'll"",
                            ""you'd"", 'your', 'yours', 'yourself', 'yourselves',
                            'he', 'him', 'his', 'himself', 'she', ""she's"",
                            'her', 'hers', 'herself', 'it', ""it's"", 'its',
                            'itself', ...]), 'vectorize__max_features': 500}","Pipeline(steps=[('preprocess', LemmatizerStemmer()),
                ('vectorize',
                 CountVectorizer(max_features=500,
                                 stop_words=['i', 'me', 'my', 'myself', 'we',
                                             'our', 'ours', 'ourselves', 'you',
                                             ""you're"", ""you've"", ""you'll"",
                                             ""you'd"", 'your', 'yours',
                                             'yourself', 'yourselves', 'he',
                                             'him', 'his', 'himself', 'she',
                                             ""she's"", 'her', 'hers', 'herself',
                                             'it', ""it's"", 'its', 'itself', ...])),
                ('classifier', MultinomialNB(alpha=0.1, fit_prior=False))])"
